{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T09:45:15.817292Z",
     "start_time": "2024-12-10T09:45:13.670479Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mpapucci/control_mgt/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from trl import (\n",
    "    DPOConfig,\n",
    "    DPOTrainer,\n",
    "    ModelConfig,\n",
    "    ScriptArguments,\n",
    "    TrlParser,\n",
    "    get_kbit_device_map,\n",
    "    get_peft_config,\n",
    "    get_quantization_config,\n",
    ")\n",
    "from trl.trainer.utils import SIMPLE_CHAT_TEMPLATE\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "import matplotlib.pyplot as plt\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from datetime import datetime\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2268997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "        \"\"\"\n",
    "        Prints the number of trainable parameters in the model.\n",
    "        \"\"\"\n",
    "        trainable_params = 0\n",
    "        all_param = 0\n",
    "        for _, param in model.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "        print(\n",
    "            f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "933ee59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_for_LoRA_training(model, accelerator):\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "            \"lm_head\",\n",
    "        ],\n",
    "        bias=\"none\",\n",
    "        lora_dropout=0.05,  # Conventional\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, config, adapter_name=\"training_adapter\")\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    # Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "    model = accelerator.prepare_model(model)\n",
    "    return model\n",
    "\n",
    "def get_quant_model(model_name):\n",
    "    bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85ff8211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accelerator():\n",
    "    fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "        state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "        optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    )\n",
    "\n",
    "    accelerator = Accelerator(fsdp_plugin=fsdp_plugin)\n",
    "    return accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d95ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        add_eos_token=True,\n",
    "        add_bos_token=True,\n",
    "        use_fast=False\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.chat_template = SIMPLE_CHAT_TEMPLATE\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b843cffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "accelerator = get_accelerator()\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model = get_quant_model(model_name)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = get_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f6051b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"data/dataset-max-feature-difference-top-10_iter_1.jsonl\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b57ca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 88121344 || all params: 4628721664 || trainable%: 1.9037944036550305\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_LoRA_training(model, accelerator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56dd8a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'config = LoraConfig(\\n        r=32,\\n        lora_alpha=16,\\n        target_modules=[\\n            \"q_proj\",\\n            \"k_proj\",\\n            \"v_proj\",\\n            \"o_proj\",\\n            \"gate_proj\",\\n            \"up_proj\",\\n            \"down_proj\",\\n            \"lm_head\",\\n        ],\\n        bias=\"none\",\\n        lora_dropout=0.05,  # Conventional\\n        task_type=\"CAUSAL_LM\",\\n    )\\n\\nmodel.add_adapter(config, adapter_name=\"reference_adapter\")'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"config = LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "            \"lm_head\",\n",
    "        ],\n",
    "        bias=\"none\",\n",
    "        lora_dropout=0.05,  # Conventional\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "model.add_adapter(config, adapter_name=\"reference_adapter\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82e505eba248f483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mpapucci/control_mgt/.venv/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/mpapucci/control_mgt/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/mpapucci/control_mgt/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23/23 07:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mpapucci/control_mgt/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=23, training_loss=0.5216325262318486, metrics={'train_runtime': 496.9279, 'train_samples_per_second': 1.477, 'train_steps_per_second': 0.046, 'total_flos': 0.0, 'train_loss': 0.5216325262318486, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the DPOConfig with your training parameters\n",
    "training_config = DPOConfig(\n",
    "    output_dir=\"model\",\n",
    "    warmup_steps=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5.0e-6,\n",
    "    bf16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_dir=\"model/logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    do_eval=True,\n",
    "    num_train_epochs=1,\n",
    "    run_name=f\"test-llama{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"\n",
    ")\n",
    "\n",
    "# Initialize the DPOTrainer with the DPOConfig\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_config,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794af79c46fcd25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
